# ğŸ“ˆ Benchmarks

Standardized test suites used to evaluate agent or multi-agent system performance.

- [AgentBench: Evaluating LLMs as Agents](https://arxiv.org/abs/2308.03688)
- [GAIA: a benchmark for General AI Assistants](https://arxiv.org/abs/2311.12983)
- [VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks](https://arxiv.org/abs/2512.16501)
- [CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society](https://arxiv.org/abs/2303.17760)
- [AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors](https://arxiv.org/abs/2308.10848)
- [SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents](https://arxiv.org/abs/2310.11667)
- [AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2504.00587)

> ğŸ’¡ Tip: Papers may also appear in other categories.
